{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = torch.tensor([7, 7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5473, 0.2097, 0.6984, 0.8675],\n",
       "         [0.6385, 0.5979, 0.2395, 0.0789],\n",
       "         [0.8791, 0.2240, 0.9851, 0.4896]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor = torch.rand(size=(3,4))\n",
    "random_tensor, random_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的梯度,  tensor([1., 2.])\n",
      "before backbard w:  tensor([0.5000, 0.5000], requires_grad=True)\n",
      "after backward w:  tensor([0.4900, 0.4800], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "w = torch.tensor([.5,.5], requires_grad=True)\n",
    "# loss = w_1^2 + 2w_2^2\n",
    "loss = w[0]**2+2*w[1]**2\n",
    "opti = torch.optim.SGD([w], lr=0.01)\n",
    "\n",
    "loss.backward()\n",
    "print(\"w的梯度, \", w.grad)\n",
    "print(\"before backbard w: \", w)\n",
    "opti.step()\n",
    "print(\"after backward w: \", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的梯度,  tensor([1., 2.])\n",
      "before backbard w:  tensor([0.5000, 0.5000], requires_grad=True)\n",
      "after backward w:  tensor([0.4900, 0.4800], requires_grad=True)\n",
      "after backward w:  tensor([0.4800, 0.4600], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 多更新一次参数，会怎样？\n",
    "import torch \n",
    "\n",
    "w=torch.tensor([0.5, .5], requires_grad=True)\n",
    "loss = w[0]**2+2*w[1]**2\n",
    "opti = torch.optim.SGD([w], lr=0.01)\n",
    "\n",
    "loss.backward()\n",
    "print(\"w的梯度, \", w.grad)\n",
    "print(\"before backbard w: \", w)\n",
    "opti.step()\n",
    "print(\"after backward w: \", w)\n",
    "# opti.step()的作用只是机械地再做一次梯度更新\n",
    "opti.step()\n",
    "print(\"after backward w: \", w)\n",
    "\n",
    "# 实际训练中，是不是这样一步一步地更新参数？\n",
    "# 并不是。更新完参数后，需要用新的参数跑下一批数据，然后得到新的loss，算出新的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的梯度,  tensor([1., 2.])\n",
      "before backbard w:  tensor([0.5000, 0.5000], requires_grad=True)\n",
      "after backward w:  tensor([0.4900, 0.4800], requires_grad=True)\n",
      "w的梯度,  tensor([0.9800, 1.9200])\n",
      "before backbard w:  tensor([0.4900, 0.4800], requires_grad=True)\n",
      "after backward w:  tensor([0.4802, 0.4608], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 更新完参数后，让梯度清零，然后重新做梯度计算\n",
    "\n",
    "import torch \n",
    "w = torch.tensor([.5, .5], requires_grad=True)\n",
    "loss = w[0]**2+2*w[1]**2\n",
    "opti = torch.optim.SGD([w], lr=0.01)\n",
    "\n",
    "loss.backward()\n",
    "print(\"w的梯度, \", w.grad)\n",
    "print(\"before backbard w: \", w)\n",
    "opti.step()\n",
    "print(\"after backward w: \", w)\n",
    "opti.zero_grad()\n",
    "\n",
    "# 这里为什么做一次clone操作呢？\n",
    "loss=w[0].clone()**2+2*w[1].clone()**2\n",
    "loss.backward()\n",
    "print(\"w的梯度, \", w.grad)\n",
    "print(\"before backbard w: \", w)\n",
    "opti.step()\n",
    "print(\"after backward w: \", w)\n",
    "opti.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的梯度,  tensor([1., 2.])\n",
      "before backbard w:  tensor([0.5000, 0.5000], requires_grad=True)\n",
      "after backward 1 step:  tensor([0.4850, 0.4750], requires_grad=True)\n",
      "after backward 2 step:  tensor([0.4702, 0.4502], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 权重值衰减\n",
    "\n",
    "import torch \n",
    "\n",
    "w = torch.tensor([.5, .5], requires_grad=True)\n",
    "loss = w[0]**2+2*w[1]**2\n",
    "opti = torch.optim.SGD([w], lr=0.01, weight_decay=1)\n",
    "\n",
    "loss.backward()\n",
    "print(\"w的梯度, \", w.grad)\n",
    "print(\"before backbard w: \", w)\n",
    "opti.step()\n",
    "print(\"after backward 1 step: \", w)\n",
    "opti.step()\n",
    "print(\"after backward 2 step: \", w)\n",
    "\n",
    "# 注意，打印梯度的时候，你会发现w的梯度没有把权值衰减考虑进去。\n",
    "# 因为权值衰减是优化器要考虑的事情，而反向传播（求梯度）不关优化器的事情。权值衰减是在优化器阶段做的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的梯度: tensor([1., 2.])\n",
      "w: tensor([0.5000, 0.5000], requires_grad=True)\n",
      "w: tensor([0.4900, 0.4800], requires_grad=True)\n",
      "w: tensor([0.4790, 0.4580], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 更新sgd的momentum\n",
    "\n",
    "import torch\n",
    "w = torch.tensor([0.5,0.5],requires_grad=True)\n",
    "loss = w[0].clone()**2 + 2*w[1].clone()**2\n",
    "opti = torch.optim.SGD([w],lr=0.01,momentum=0.1)\n",
    "\n",
    "loss.backward(retain_graph=True)\n",
    "print(\"w的梯度:\",w.grad)\n",
    "print(\"w:\",w)\n",
    "\n",
    "opti.step()\n",
    "print(\"w:\",w)\n",
    "\n",
    "opti.step()\n",
    "print(\"w:\",w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的梯度: tensor([1., 2.])\n",
      "w: tensor([0.5000, 0.5000], requires_grad=True)\n",
      "w: tensor([0.4900, 0.4900], requires_grad=True)\n",
      "w的梯度: tensor([0.9800, 1.9600])\n",
      "w: tensor([0.4830, 0.4830], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 使用不同的优化器\n",
    "# SGD对于每一个参数都使用同一个学习率。\n",
    "# adagrad则对每一个参数使用不同的学习率：梯度越大，学习率越小；梯度越小，学习率越大\n",
    "\n",
    "import torch\n",
    "w = torch.tensor([0.5,0.5],requires_grad=True)\n",
    "loss = w[0].clone()**2 + 2*w[1].clone()**2\n",
    "opti = torch.optim.Adagrad([w],lr=0.01,lr_decay=0, weight_decay=0, initial_accumulator_value=0)\n",
    "\n",
    "loss.backward(retain_graph=True)\n",
    "print(\"w的梯度:\",w.grad)\n",
    "print(\"w:\",w)\n",
    "\n",
    "opti.step()\n",
    "print(\"w:\",w)\n",
    "\n",
    "opti.zero_grad()\n",
    "\n",
    "loss = w[0].clone()**2 + 2*w[1].clone()**2\n",
    "loss.backward(retain_graph=True)\n",
    "print(\"w的梯度:\",w.grad)\n",
    "\n",
    "opti.step()\n",
    "print(\"w:\",w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
