{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯实践——文本分类\n",
    "\n",
    "之所以称之为朴素贝叶斯，是因为它假设每个输入变量都是独立的。这是一个强硬的假设，实际情况并不一定，但是这项技术对于绝大部分的复杂问题仍然的有效的。\n",
    "\n",
    "朴素贝叶斯分类常用于文本分类，尤其是对于英文等语言来说，分类效果很好。它常用于垃圾文本过滤、情感预测、推荐系统等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载停止词\n",
    "\n",
    "所谓停止词，就是不能帮助区分文档类型的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停止词\n",
    "stop_words = [line.strip() for line in open('./stop/stopword.txt', 'r', encoding='utf-8').readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词&打标\n",
    "\n",
    "中文一般使用jieba进行分词，英文一般使用nltk进行分词 \n",
    "\n",
    "分词与打标的结果：\n",
    "[ 文件1的分词，中间以空格隔开]：label\n",
    "[ 文件2的分词，中间以空格隔开]：label\n",
    "...\n",
    "[ 文件n的分词，中间以空格隔开]：label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词与打标\n",
    "\n",
    "import os \n",
    "import jieba \n",
    "\n",
    "def cut_words(file_path):\n",
    "    words = []\n",
    "    with open(file_path, 'r', encoding='gb18030') as file:\n",
    "        text = file.read()\n",
    "        words = [word for word in jieba.cut(text)]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "def loadfile(file_dir, label):\n",
    "    \"\"\"\n",
    "    将路径下的所有文件加载\n",
    "    :param file_dir: 保存txt文件目录\n",
    "    :param label: 文档标签\n",
    "    :return: 分词后的文档列表和标签\n",
    "    \"\"\"\n",
    "    file_list = os.listdir(file_dir)\n",
    "    words_list = []\n",
    "    labels_list = []\n",
    "    for file in file_list:\n",
    "        file_path = file_dir + '/' + file\n",
    "        words_list.append(cut_words(file_path))\n",
    "        labels_list.append(label)                                                                                                                 \n",
    "    return words_list, labels_list\n",
    "\n",
    "# 加载训练数据\n",
    "train_words_list1, train_labels1 = loadfile('./train/女性', '女性')\n",
    "train_words_list2, train_labels2 = loadfile('./train/体育', '体育')\n",
    "train_words_list3, train_labels3 = loadfile('./train/文学', '文学')\n",
    "train_words_list4, train_labels4 = loadfile('./train/校园', '校园')\n",
    "\n",
    "train_words_list = train_words_list1 + train_words_list2 + train_words_list3 + train_words_list4    \n",
    "train_labels = train_labels1 + train_labels2 + train_labels3 + train_labels4\n",
    "\n",
    "# 加载测试数据\n",
    "test_words_list1, test_labels1 = loadfile('./test/女性', '女性')\n",
    "test_words_list2, test_labels2 = loadfile('./test/体育', '体育')\n",
    "test_words_list3, test_labels3 = loadfile('./test/文学', '文学')\n",
    "test_words_list4, test_labels4 = loadfile('./test/校园', '校园')\n",
    "\n",
    "test_words_list = test_words_list1 + test_words_list2 + test_words_list3 + test_words_list4\n",
    "test_labels = test_labels1 + test_labels2 + test_labels3 + test_labels4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算单词权重\n",
    "\n",
    "使用tf-idf计算单词权重，max_df=0.5表示单词在文档中出现的频率超过50%的文档将被忽略 \n",
    "\n",
    "TF：单词在一个文件出现的频率\n",
    "\n",
    "$$\n",
    "TF = \\frac{单词在文件中出现的次数}{文件中单词总数}\n",
    "$$\n",
    "\n",
    "IDF：单词在所有文件中出现的频率\n",
    "$$\n",
    "IDF = \\log(\\frac{文件总数}{包含单词的文件数 + 1})\n",
    "$$\n",
    "\n",
    "TF-IDF：TF与IDF的乘积。其实就是综合考虑TF和IDF，TF高的单词，可能是没啥区分度的单词，比如'我'，'的'，'是'等。 IDF高的单词，说明就是区分度很高的。\n",
    "$$\n",
    "TF-IDF = TF \\times IDF\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算单词权重\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5)\n",
    "train_features = tf.fit_transform(train_words_list)\n",
    "test_features = tf.transform(test_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多项式贝叶斯分类器\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测&计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91\n"
     ]
    }
   ],
   "source": [
    "# 预测&计算准确率\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predicted_labels = clf.predict(test_features)\n",
    "print(accuracy_score(test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
